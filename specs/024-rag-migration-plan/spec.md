# Feature Specification: RAG Migration Plan

**Feature Branch**: `001-rag-migration-plan`
**Created**: 2026-01-18
**Status**: Draft
**Input**: User description: "RAG migration plan from Ollama-based local LLM processing to RAG framework (Haystack or txtai)"

## Overview

現在の Obsidian ナレッジベースは、Ollama を使用したローカル LLM 処理でコンテンツの要約・抽出を行っている。本機能では、RAG（Retrieval-Augmented Generation）フレームワークを導入し、ナレッジベース全体に対するセマンティック検索と質問応答機能を実現する。

### Goals

- ナレッジベース内の情報を自然言語で検索できる
- 質問に対して関連ドキュメントを参照した回答を得られる
- 関連するナレッジの発見・関連付けを支援する

### Non-Goals

- 既存の llm_import / normalizer パイプラインの置き換え（共存する）
- クラウドサービスへの依存（ローカル環境で完結）
- リアルタイム同期（バッチ処理で十分）

## User Scenarios & Testing *(mandatory)*

### User Story 1 - ナレッジベース検索 (Priority: P1)

ユーザーとして、自然言語でナレッジベースを検索し、関連するドキュメントを見つけたい。

**Why this priority**: ナレッジベースの最も基本的な価値は「必要な情報を見つけられること」。キーワード検索では見つからない概念的な関連性を発見できることが RAG 導入の核心価値。

**Independent Test**: 「Kubernetes のデプロイ戦略について」と検索し、関連するエンジニアリングノートが返されることを確認できる。

**Acceptance Scenarios**:

1. **Given** ナレッジベースにドキュメントがインデックス済み, **When** ユーザーが自然言語で検索クエリを入力, **Then** 意味的に関連するドキュメントのリストが関連度順に表示される
2. **Given** 検索結果が表示されている, **When** ユーザーがドキュメントを選択, **Then** 該当ドキュメントのファイルパスとスニペット（検索クエリ周辺のテキスト抜粋）が表示される
3. **Given** 検索クエリに一致するドキュメントがない, **When** ユーザーが検索を実行, **Then** 「該当するドキュメントが見つかりません」と表示される *(類似候補提案は将来拡張)*

---

### User Story 2 - 質問応答（Q&A） (Priority: P2)

ユーザーとして、ナレッジベースの内容に基づいた質問をし、回答と出典を得たい。

**Why this priority**: 検索だけでなく、複数ドキュメントを統合した回答が得られることで、ナレッジの活用度が大幅に向上する。ただし検索機能が前提となるため P2。

**Independent Test**: 「OAuth2 と JWT の違いは？」と質問し、ナレッジベース内のドキュメントを引用した回答が得られることを確認できる。

**Acceptance Scenarios**:

1. **Given** ナレッジベースにドキュメントがインデックス済み, **When** ユーザーが質問を入力, **Then** ナレッジベースの内容に基づいた回答が生成され、参照元ドキュメントへのリンクが表示される
2. **Given** 質問がナレッジベースの範囲外, **When** ユーザーが質問を入力, **Then** 「この質問に答えるための情報がナレッジベースにありません」と明示される
3. **Given** 回答が生成された, **When** ユーザーが出典リンクをクリック, **Then** 参照元ドキュメントの該当箇所に遷移する

---

### User Story 3 - インデックス更新 (Priority: P3)

ユーザーとして、新しく追加・更新されたドキュメントを検索対象に含めたい。

**Why this priority**: 検索・Q&A 機能が動作した後、継続的な運用のために必要。初期リリースでは手動でのインデックス更新でも許容可能。

**Independent Test**: 新規ドキュメントを追加後、インデックス更新を実行し、そのドキュメントが検索結果に表示されることを確認できる。

**Acceptance Scenarios**:

1. **Given** 新しいドキュメントが Vault に追加された, **When** ユーザーがインデックス更新を実行, **Then** 新しいドキュメントが検索対象に追加される
2. **Given** 既存ドキュメントが更新された, **When** ユーザーがインデックス更新を実行, **Then** 更新内容が検索インデックスに反映される
3. **Given** ドキュメントが削除された, **When** ユーザーがインデックス更新を実行, **Then** 削除されたドキュメントが検索対象から除外される

---

### Edge Cases

- 大量のドキュメント（1000件以上）がある場合のインデックス作成時間は？
- 日本語と英語が混在するドキュメントの検索精度は？
- 画像を含むドキュメントのテキスト以外のコンテンツはどう扱うか？
- インデックス作成中に検索が実行された場合の動作は？

## Requirements *(mandatory)*

### Functional Requirements

- **FR-001**: システムは Vault 内の Markdown ドキュメントをインデックス化できなければならない
- **FR-002**: システムは自然言語クエリを受け付け、意味的に関連するドキュメントを返却できなければならない
- **FR-003**: システムは検索結果を関連度スコアでソートして表示できなければならない
- **FR-004**: システムは質問に対してナレッジベースを参照した回答を生成できなければならない
- **FR-005**: システムは回答の出典（参照元ドキュメントと該当箇所）を提示できなければならない
- **FR-006**: システムはローカル環境で完結し、外部クラウドサービスへのデータ送信を行わない
- **FR-007**: システムはインデックスの増分更新（追加・更新・削除）をサポートしなければならない
- **FR-008**: システムは日本語と英語の両方のドキュメントを適切に処理できなければならない
- **FR-009**: システムは既存の llm_import / normalizer パイプラインと共存できなければならない

### Key Entities

- **Document**: ナレッジベース内の Markdown ファイル。タイトル、本文、frontmatter メタデータ、Vault 分類を持つ
- **Index**: ドキュメントのベクトル表現を格納する検索用データ構造。Document と 1:N の関係（チャンク分割）
- **Query**: ユーザーからの検索クエリまたは質問。自然言語テキスト
- **SearchResult**: 検索結果。Document への参照、関連度スコア、該当箇所のスニペットを含む
- **Answer**: Q&A の回答。生成テキストと出典（SearchResult のリスト）を含む

## Success Criteria *(mandatory)*

### Measurable Outcomes

- **SC-001**: ユーザーは検索クエリ入力から結果表示まで 3 秒以内に完了できる（1000ドキュメント規模）
- **SC-002**: 検索結果の上位 5 件に関連ドキュメントが含まれる精度が 80% 以上
  - 測定方法: 10件のテストクエリに対し、人間が判定した「関連あり」ドキュメントが top-5 に1件以上含まれる割合
- **SC-003**: Q&A 回答の出典引用が正確である（回答内容と出典の整合性）割合が 90% 以上
  - 測定方法: 10件のテスト質問に対し、回答内の主張が引用元ドキュメントに実際に記載されているかを人間が検証
- **SC-004**: 全 Vault のインデックス作成が 10 分以内に完了する（1000ドキュメント規模）
- **SC-005**: 増分インデックス更新が 1 ドキュメントあたり 1 秒以内に完了する
- **SC-006**: システムは 8GB メモリ環境で安定して動作する

## Assumptions

- Ollama がローカル環境にインストール済みで、embedding モデルが利用可能
- 対象ドキュメントは主に Markdown 形式（画像埋め込みは対象外）
- ユーザーは CLI または Makefile 経由でシステムを操作する（GUI は対象外）
- 1000 ドキュメント程度の規模を想定（大規模エンタープライズ用途は対象外）

## Technical Decisions

### Framework

- **RAG Framework**: Haystack 2.x
  - 選定理由: 監査・可観測性が充実、OpenTelemetry 対応、規制業界での実績
- **Logging**: structlog
  - 選定理由: 構造化ログで運用時のデバッグ効率向上、既存パイプラインとの統一

### Infrastructure

| コンポーネント | 選択 | ホスト | 備考 |
|---------------|------|--------|------|
| **Document Store** | Qdrant (ローカル永続化) | localhost | フィルタリング・永続化対応 |
| **Embedding** | bge-m3 | pi@ollama-server.local (NanoPC-T6) | 日本語最適化、1024次元 |
| **LLM** | gpt-oss:20b | localhost | 既存パイプラインと統一 |

### Embedding Server (Remote)

- **Host**: `ollama-server.local` (NanoPC-T6, ARM64, 16GB RAM)
- **Ollama Version**: 0.14.2
- **Model**: bge-m3 (trilingual: 日本語・英語・中国語)
- **Endpoint**: `http://ollama-server.local:11434/api/embeddings`

### Code Location

- **実装ディレクトリ**: `/src/rag/`
- **既存パイプラインとの関係**: `development/scripts/` と共存

## Scope

### In Scope

- Vault 内 Markdown ドキュメントのインデックス作成
- セマンティック検索機能
- RAG ベースの Q&A 機能
- CLI / Makefile インターフェース
- ローカル LLM（Ollama）との統合

### Out of Scope

- Web UI / GUI
- リアルタイム同期・ファイル監視
- 画像・PDF などの非テキストコンテンツ処理
- マルチユーザー対応
- クラウドサービス連携
