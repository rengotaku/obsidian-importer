# Feature Specification: 大規模ファイルのチャンク分割処理

**Feature Branch**: `020-large-file-chunking`
**Created**: 2026-01-17
**Status**: Draft
**Input**: User description: "ファイルサイズが大きい場合は、分割して処理させる方式にする。"

## User Scenarios & Testing *(mandatory)*

### User Story 1 - 大規模会話の自動分割処理 (Priority: P1)

ユーザーが LLM インポート処理を実行する際、コンテキストサイズを超える大規模な会話ファイルが自動的にチャンク分割され、各チャンクが個別に処理された後、結果が統合される。これにより、ファイルサイズに関係なく全ての会話が正常に処理される。

**Why this priority**: 現在、大規模ファイルがコンテキストオーバーフローでエラーになっている根本問題の解決。この機能がないと処理が完了しない。

**Independent Test**: 50,000文字以上の会話ファイルを処理し、エラーなく完了することを確認。

**Acceptance Scenarios**:

1. **Given** 100,000文字を超える会話ファイル, **When** インポート処理を実行する, **Then** ファイルが自動的にチャンク分割され、各チャンクが個別に処理される
2. **Given** チャンク分割された会話, **When** 全チャンクの処理が完了する, **Then** 各チャンクが連番付きの個別ファイルとして出力される
3. **Given** 分割処理中にエラーが発生, **When** 特定のチャンクが失敗する, **Then** 失敗したチャンクのみリトライ対象となり、成功したチャンクは保持される

---

### User Story 2 - 処理進捗の可視化 (Priority: P2)

大規模ファイルの分割処理中、ユーザーは現在どのチャンクを処理しているか、全体の進捗状況を確認できる。

**Why this priority**: ユーザー体験の向上。大規模ファイル処理は時間がかかるため、進捗が見えることで安心感を提供。

**Independent Test**: 分割処理中のログ出力で、チャンク番号と全体進捗が表示されることを確認。

**Acceptance Scenarios**:

1. **Given** チャンク分割処理が開始された, **When** 各チャンクの処理が進む, **Then** ログに「チャンク N/M 処理中」のような進捗が表示される

---

### Edge Cases

- 会話がチャンク境界で意味的に分断される場合: 文脈を維持するためのオーバーラップ領域を設ける
- 単一メッセージがチャンクサイズを超える場合: そのメッセージ単独で1チャンクとして処理
- 空のチャンクが生成される場合: スキップして次のチャンクへ進む

## Requirements *(mandatory)*

### Functional Requirements

- **FR-001**: システムは、会話の総文字数がチャンクサイズ閾値（デフォルト: 30,000文字）を超える場合、自動的にチャンク分割を行わなければならない
- **FR-002**: チャンク分割は、メッセージ境界で行わなければならない（メッセージの途中で分割しない）
- **FR-003**: 各チャンクには、文脈維持のため前のチャンクの末尾部分（オーバーラップ: 2,000文字）を含めなければならない
- **FR-004**: 各チャンクの LLM 処理結果は、連番付きの個別ファイルとして出力されなければならない（例: `タイトル_001.md`, `タイトル_002.md`）
- **FR-005**: チャンク処理の進捗は、ログに出力されなければならない

### Key Entities

- **Chunk（チャンク）**: 分割された会話の一部分。開始位置、終了位置、オーバーラップ情報、処理状態を持つ
- **ChunkResult（チャンク結果）**: 各チャンクの LLM 処理結果。サマリー、学び、コードスニペットを含む

## Success Criteria *(mandatory)*

### Measurable Outcomes

- **SC-001**: 100,000文字を超える会話ファイルが、エラーなく正常に処理される
- **SC-002**: チャンク分割による処理時間増加は、分割なしの場合と比較して2倍以内
- **SC-003**: 処理失敗率が、大規模ファイルにおいて現在の100%から10%以下に改善される

## Assumptions

- チャンクサイズ閾値 30,000 文字は、LLM のコンテキストサイズ 16,384 トークンに基づく概算（日本語1文字 ≈ 1-2トークン）
- オーバーラップ 2,000 文字は、文脈維持に十分な量として設定
- 各チャンクは独立したファイルとして出力されるため、統合処理は不要
