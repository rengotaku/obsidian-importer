#!/usr/bin/env python3
"""V5 æœ€çµ‚æ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆè¡¨ã‚’å¾Œã‹ã‚‰è¿½åŠ ã™ã‚‹æ–¹å¼ï¼‰"""

import sys

sys.path.insert(0, "/data/projects/obsidian-importer/specs/052-improve-summary-quality")

from pathlib import Path

import requests
from marker_preprocessor import get_marker_prompt_instruction, postprocess, preprocess

BASE_DIR = Path(__file__).parent / "verification-outputs"
PROMPT_FILE = Path(__file__).parent / "v3-prompt.txt"

SAMPLES = [
    "S1-389c1d35f44f",
    "S2-edb42c441a83",
    "S3-8b8869107b00",
    "S4-1b1679e5dd57",
]

OLLAMA_URL = "http://localhost:11434/api/generate"
MODEL = "gpt-oss:20b"


def call_ollama(prompt: str, content: str) -> str:
    """Ollama API ã‚’å‘¼ã³å‡ºã—ã¦çµæœã‚’è¿”ã™"""
    full_prompt = f"{prompt}\n\n---\n\nä»¥ä¸‹ã®ä¼šè©±ãƒ­ã‚°ã‹ã‚‰çŸ¥è­˜ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„:\n\n{content}"

    payload = {
        "model": MODEL,
        "prompt": full_prompt,
        "stream": False,
        "options": {
            "temperature": 0.2,
            "num_predict": 16384,
        },
    }

    response = requests.post(OLLAMA_URL, json=payload, timeout=300)
    response.raise_for_status()
    return response.json().get("response", "")


def main():
    base_prompt = PROMPT_FILE.read_text(encoding="utf-8")
    marker_instruction = get_marker_prompt_instruction()
    prompt = base_prompt + "\n" + marker_instruction

    results = []

    for sample in SAMPLES:
        sample_dir = BASE_DIR / sample
        original_file = sample_dir / "original.md"
        output_file = sample_dir / "v5-output.md"

        if not original_file.exists():
            print(f"âš ï¸ {sample}: original.md not found")
            continue

        print(f"ğŸ”„ {sample}: Processing...")

        content = original_file.read_text(encoding="utf-8")

        # å‰å‡¦ç†
        pre_result = preprocess(content)
        code_count = len(pre_result.code_markers)
        table_count = len(pre_result.extracted_tables)

        # LLMå‘¼ã³å‡ºã—
        llm_output = call_ollama(prompt, pre_result.processed_text)

        # å¾Œå‡¦ç†
        post_result = postprocess(llm_output, pre_result)

        output_file.write_text(post_result.output, encoding="utf-8")

        # çµ±è¨ˆ
        original_len = len(content)
        output_len = len(post_result.output)
        ratio = (output_len / original_len * 100) if original_len > 0 else 0

        status = "ğŸ” REVIEW" if post_result.needs_review else "âœ… OK"
        extras = []
        if code_count > 0:
            extras.append(f"code:{code_count}")
        if table_count > 0:
            extras.append(f"table:{table_count}")
        if post_result.fallback_used:
            extras.append("fallback")
        extra_str = f" ({', '.join(extras)})" if extras else ""

        print(f"{status} {sample}: {original_len} â†’ {output_len} chars ({ratio:.1f}%){extra_str}")

        results.append(
            {
                "sample": sample,
                "original": original_len,
                "output": output_len,
                "ratio": ratio,
                "needs_review": post_result.needs_review,
                "code_count": code_count,
                "table_count": table_count,
                "markers_found": len(post_result.markers_found),
                "markers_missing": len(post_result.markers_missing),
            }
        )

    # ã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 70)
    print("V5 æœ€çµ‚æ¤œè¨¼çµæœ")
    print("=" * 70)
    print(
        f"{'Sample':<20} {'å…ƒ':<8} {'å‡ºåŠ›':<8} {'åœ§ç¸®ç‡':<8} {'Code':<6} {'Table':<6} {'çŠ¶æ…‹':<10}"
    )
    print("-" * 70)
    for r in results:
        status = "REVIEW" if r["needs_review"] else "OK"
        print(
            f"{r['sample']:<20} {r['original']:<8} {r['output']:<8} "
            f"{r['ratio']:.1f}%{'':<4} {r['code_count']:<6} {r['table_count']:<6} {status:<10}"
        )

    print("-" * 70)
    review_count = sum(1 for r in results if r["needs_review"])
    print(f"REVIEW è¡Œã: {review_count}/{len(results)}")


if __name__ == "__main__":
    main()
