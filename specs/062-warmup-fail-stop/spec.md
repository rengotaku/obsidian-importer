# 機能仕様書: ウォームアップ失敗時に処理を停止する

**Feature Branch**: `062-warmup-fail-stop`
**Created**: 2026-02-25
**Status**: Draft
**Input**: GitHub Issue #41 - ウォームアップ失敗時に処理を停止する

## 背景と目的

Ollama モデルのウォームアップがタイムアウトした場合、現在は WARNING を出力して処理を続行している。これにより、モデルが未ロード状態で LLM 呼び出しが行われ、空レスポンスが返される問題が発生している。

本機能では、ウォームアップ失敗時に即座に処理を停止し、ユーザーに明確なエラーを通知することで、後続の連鎖的な障害を防止する。

## ユーザーシナリオとテスト

### ユーザーストーリー 1 - ウォームアップ失敗時の即時停止 (Priority: P1)

パイプライン実行者として、Ollama モデルのウォームアップが失敗した場合、処理が即座に停止してほしい。空レスポンスによる連鎖的なエラーを防ぎ、問題の原因を即座に特定できるようにするため。

**Why this priority**: ウォームアップ失敗後に処理を続行しても、すべての LLM 呼び出しが失敗するため、早期に停止することが最も重要。

**Independent Test**: ウォームアップがタイムアウトする状況を再現し、処理が停止することを確認できる。

**受け入れシナリオ**:

1. **Given** Ollama サーバーが応答遅延している状態, **When** パイプラインを実行する, **Then** ウォームアップがタイムアウトし、ERROR ログが出力されて処理が停止する
2. **Given** Ollama サーバーが停止している状態, **When** パイプラインを実行する, **Then** 接続エラーとして ERROR ログが出力され、処理が停止する
3. **Given** ウォームアップが失敗した状態, **When** 処理が停止する, **Then** 終了コード 3（Ollama 接続エラー）が返される

---

### ユーザーストーリー 2 - 明確なエラーメッセージの表示 (Priority: P2)

パイプライン実行者として、ウォームアップ失敗時に何が問題かを明確に理解したい。問題を迅速に解決できるようにするため。

**Why this priority**: 処理停止後、ユーザーが問題を理解し対処できることが重要。

**Independent Test**: エラーメッセージの内容を確認し、問題の原因が明確かどうかを検証できる。

**受け入れシナリオ**:

1. **Given** ウォームアップがタイムアウトした状態, **When** エラーメッセージを確認する, **Then** タイムアウトしたモデル名と推奨アクション（Ollama サーバーの確認）が表示される
2. **Given** Ollama サーバーに接続できない状態, **When** エラーメッセージを確認する, **Then** 接続先のホスト情報と推奨アクション（サーバー起動の確認）が表示される

---

### エッジケース

- ウォームアップ中にネットワーク切断が発生した場合 → 接続エラーとして処理停止
- 複数モデルを使用する設定で、一部のモデルのみウォームアップ失敗した場合 → 最初の失敗時点で処理停止
- リトライ設定がある場合 → リトライ上限到達後に処理停止

## 要件

### 機能要件

- **FR-001**: システムは、Ollama モデルのウォームアップがタイムアウトした場合、ERROR レベルでログを出力しなければならない
- **FR-002**: システムは、ウォームアップ失敗時に即座に処理を停止しなければならない
- **FR-003**: システムは、ウォームアップ失敗時に終了コード 3（Ollama 接続エラー）を返さなければならない
- **FR-004**: エラーメッセージには、失敗したモデル名とタイムアウト情報を含めなければならない
- **FR-005**: エラーメッセージには、問題解決のための推奨アクションを含めなければならない

## 成功基準

### 測定可能な結果

- **SC-001**: ウォームアップ失敗時、後続の LLM 呼び出しが一切行われないこと
- **SC-002**: ウォームアップ失敗から処理停止まで 1 秒以内に完了すること
- **SC-003**: エラーメッセージを見たユーザーが、問題の原因を特定できること（モデル名、タイムアウト理由が明記）
- **SC-004**: 終了コード 3 が正しく返され、CI/CD パイプラインでのエラー検出が可能であること

## 前提条件

- Ollama サーバーのウォームアップ処理は既存の `ollama.py:47` 付近で実装されている
- 終了コード 3 は既にプロジェクトで「Ollama 接続エラー」として定義されている
- 現在の WARNING ログを ERROR に変更することで、ログレベルの一貫性は維持される

## スコープ外

- ウォームアップのリトライ回数やタイムアウト値の設定変更
- 複数モデルの部分的ウォームアップ成功時の継続処理
- Ollama 以外の LLM プロバイダーへのフォールバック機能
