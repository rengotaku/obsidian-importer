#!/usr/bin/env python3
"""
Claude Export Importer
Claude ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’è§£æã—ã€Obsidian ãƒãƒ¼ãƒˆã¨ã—ã¦é©åˆ‡ãª Vault ã«æŒ¯ã‚Šåˆ†ã‘ã‚‹

Usage:
    python import_claude_conversations.py <export_dir> [--dry-run]
"""

import json
import os
import argparse
from datetime import datetime
from pathlib import Path
import re
from typing import Tuple


# åˆ†é¡ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
CLASSIFICATION_RULES = {
    'Engineering': {
        'keywords': [
            # ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èª
            'python', 'javascript', 'typescript', 'ruby', 'go', 'rust', 'java',
            'rails', 'react', 'vue', 'node', 'docker', 'kubernetes', 'k8s',
            # æŠ€è¡“ç”¨èª
            'api', 'database', 'sql', 'git', 'github', 'cli', 'terminal',
            'server', 'nginx', 'linux', 'ubuntu', 'systemd', 'ssh',
            'code', 'function', 'class', 'debug', 'error', 'bug',
            'deploy', 'ci/cd', 'devops', 'aws', 'cloud',
            # æ—¥æœ¬èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            'ãƒ—ãƒ­ã‚°ãƒ©', 'é–‹ç™º', 'ã‚³ãƒ¼ãƒ‰', 'ã‚µãƒ¼ãƒãƒ¼', 'ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹',
            'ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«', 'ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—', 'è¨­å®š', 'ã‚¨ãƒ©ãƒ¼', 'ãƒã‚°',
            'ã‚³ãƒãƒ³ãƒ‰', 'ã‚¿ãƒ¼ãƒŸãƒŠãƒ«', 'å®Ÿè£…', 'ãƒ†ã‚¹ãƒˆ',
        ],
        'weight': 1.0,
    },
    'ãƒ“ã‚¸ãƒã‚¹æ›¸': {
        'keywords': [
            # ãƒ“ã‚¸ãƒã‚¹ã‚¹ã‚­ãƒ«
            'ãƒ“ã‚¸ãƒã‚¹', 'ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆ', 'ãƒªãƒ¼ãƒ€ãƒ¼ã‚·ãƒƒãƒ—', 'çµŒå–¶',
            'ãƒ—ãƒ¬ã‚¼ãƒ³', 'äº¤æ¸‰', 'ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³', 'è©±ã—æ–¹',
            'ä¼šè­°', 'ãƒ•ã‚¡ã‚·ãƒªãƒ†', 'ã‚­ãƒ£ãƒªã‚¢', 'è»¢è·', 'å°±è·',
            'è‡ªå·±å•“ç™º', 'ç¿’æ…£', 'ç”Ÿç”£æ€§', 'æ™‚é–“ç®¡ç†',
            # æ›¸ç±é–¢é€£
            'æœ¬', 'æ›¸ç±', 'èª­æ›¸', 'è¦ç´„', 'ã‚µãƒãƒª',
            'management', 'leadership', 'career', 'productivity',
        ],
        'weight': 1.0,
    },
    'çµŒæ¸ˆ': {
        'keywords': [
            # çµŒæ¸ˆãƒ»é‡‘è
            'çµŒæ¸ˆ', 'é‡‘è', 'æŠ•è³‡', 'æ ª', 'ç‚ºæ›¿', 'å††å®‰', 'å††é«˜',
            'nisa', 'ideco', 'è³‡ç”£', 'é‹ç”¨', 'é…å½“', 'åˆ©å›ã‚Š',
            'å¸‚å ´', 'ãƒãƒ¼ã‚±ãƒƒãƒˆ', 'æ™¯æ°—', 'gdp', 'ã‚¤ãƒ³ãƒ•ãƒ¬',
            # ä¼æ¥­ãƒ»æ¥­ç•Œ
            'æ±ºç®—', 'æ¥­ç¸¾', 'ä¼æ¥­', 'ä¼šç¤¾', 'ipo', 'm&a',
            # æ”¿ç­–
            'æ”¿ç­–', 'è¦åˆ¶', 'é‡‘åˆ©', 'æ—¥éŠ€', 'fed',
            'economy', 'finance', 'investment', 'market',
        ],
        'weight': 1.0,
    },
}


def sanitize_filename(name: str, max_length: int = 80) -> str:
    """ãƒ•ã‚¡ã‚¤ãƒ«åã¨ã—ã¦å®‰å…¨ãªæ–‡å­—åˆ—ã«å¤‰æ›"""
    invalid_chars = r'[<>:"/\\|?*\n\r\t]'
    safe = re.sub(invalid_chars, '_', name)
    safe = re.sub(r'_+', '_', safe)
    safe = safe.strip(' _')
    if len(safe) > max_length:
        safe = safe[:max_length].rsplit('_', 1)[0]
    return safe or 'untitled'


def format_datetime(dt_str: str) -> str:
    """ISOå½¢å¼ã®æ—¥æ™‚ã‚’èª­ã¿ã‚„ã™ã„å½¢å¼ã«å¤‰æ›"""
    try:
        dt = datetime.fromisoformat(dt_str.replace('Z', '+00:00'))
        return dt.strftime('%Y-%m-%d %H:%M')
    except:
        return dt_str


def classify_conversation(name: str, summary: str, messages: list) -> Tuple[str, float]:
    """ä¼šè©±ã®å†…å®¹ã‹ã‚‰ã‚¸ãƒ£ãƒ³ãƒ«ã‚’åˆ†é¡"""
    # ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
    text_parts = [name.lower(), summary.lower()]
    for msg in messages[:5]:  # æœ€åˆã®5ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½¿ç”¨
        text_parts.append(msg.get('text', '').lower())
    full_text = ' '.join(text_parts)

    scores = {}
    for category, rules in CLASSIFICATION_RULES.items():
        score = 0
        for keyword in rules['keywords']:
            if keyword.lower() in full_text:
                score += 1
        scores[category] = score * rules['weight']

    if not scores or max(scores.values()) == 0:
        return ('æœªåˆ†é¡', 0.0)

    best_category = max(scores, key=scores.get)
    confidence = scores[best_category] / (sum(scores.values()) + 1)

    return (best_category, confidence)


def generate_title_from_messages(messages: list, max_length: int = 50) -> str:
    """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆ"""
    for msg in messages:
        if msg.get('sender') == 'human':
            text = msg.get('text', '').strip()
            if text:
                first_line = text.split('\n')[0].strip()
                if len(first_line) > max_length:
                    first_line = first_line[:max_length].rsplit(' ', 1)[0] + '...'
                return first_line
    return 'Untitled Conversation'


def generate_conversation_md(name: str, uuid: str, summary: str,
                            created_at: str, updated_at: str,
                            messages: list, category: str) -> str:
    """ä¼šè©±ã®Markdownã‚’ç”Ÿæˆ"""
    tags = ['claude-export', 'conversation', category.lower().replace('æ›¸', '')]

    md = f"""---
title: "{name}"
uuid: {uuid}
created: {created_at[:10] if created_at else ''}
updated: {updated_at[:10] if updated_at else ''}
category: {category}
tags:
{chr(10).join(f'  - {tag}' for tag in tags)}
message_count: {len(messages)}
---

# {name}

"""

    if summary:
        md += f"""> [!summary] Summary
> {summary[:500]}{'...' if len(summary) > 500 else ''}

"""

    if messages:
        md += "## Conversation\n\n"
        for msg in messages:
            sender = msg.get('sender', 'unknown')
            text = msg.get('text', '')
            timestamp = msg.get('created_at', '')

            icon = 'ğŸ‘¤' if sender == 'human' else 'ğŸ¤–'
            time_str = format_datetime(timestamp) if timestamp else ''

            md += f"### {icon} {sender.capitalize()}"
            if time_str:
                md += f" ({time_str})"
            md += "\n\n"

            if text:
                md += text + "\n\n"

    return md


def process_conversations(data: list, base_dir: Path, dry_run: bool = False) -> dict:
    """ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã—ã¦é©åˆ‡ãªVaultã«æŒ¯ã‚Šåˆ†ã‘"""
    stats = {
        'total': len(data),
        'by_category': {},
        'conversations': [],
    }

    for conv in data:
        uuid = conv.get('uuid', 'unknown')
        name = conv.get('name', '').strip()
        summary = conv.get('summary', '')
        created_at = conv.get('created_at', '')
        updated_at = conv.get('updated_at', '')
        messages = conv.get('chat_messages', [])

        # ã‚¿ã‚¤ãƒˆãƒ«ãŒç©ºã®å ´åˆã€æœ€åˆã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰ç”Ÿæˆ
        if not name:
            name = generate_title_from_messages(messages)

        # åˆ†é¡
        category, confidence = classify_conversation(name, summary, messages)

        # çµ±è¨ˆæ›´æ–°
        stats['by_category'][category] = stats['by_category'].get(category, 0) + 1

        # å‡ºåŠ›å…ˆæ±ºå®š
        if category == 'æœªåˆ†é¡':
            output_dir = base_dir / '@index' / 'claude' / 'uncategorized'
        else:
            output_dir = base_dir / category / 'claude-conversations'

        # ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
        date_prefix = created_at[:10] if created_at else ''
        filename = sanitize_filename(f"{date_prefix}_{name}")
        filepath = output_dir / f"{filename}.md"

        # é‡è¤‡å›é¿
        counter = 1
        base_filepath = filepath
        while filepath.exists():
            filepath = output_dir / f"{filename}_{counter}.md"
            counter += 1

        # Markdownç”Ÿæˆ
        content = generate_conversation_md(
            name=name,
            uuid=uuid,
            summary=summary,
            created_at=created_at,
            updated_at=updated_at,
            messages=messages,
            category=category
        )

        stats['conversations'].append({
            'name': name,
            'uuid': uuid,
            'category': category,
            'confidence': confidence,
            'file': str(filepath.relative_to(base_dir)),
            'created': created_at,
        })

        if not dry_run:
            output_dir.mkdir(parents=True, exist_ok=True)
            filepath.write_text(content, encoding='utf-8')

    return stats


def generate_import_report(stats: dict, base_dir: Path, dry_run: bool = False) -> str:
    """ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    report = f"""---
title: Claude Import Report
created: {datetime.now().strftime('%Y-%m-%d')}
tags:
  - claude-export
  - report
---

# Claude Import Report

{'**DRY RUN** - å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ä½œæˆã•ã‚Œã¦ã„ã¾ã›ã‚“' if dry_run else ''}

## Statistics

- **Total Conversations**: {stats['total']}
- **Import Date**: {datetime.now().strftime('%Y-%m-%d %H:%M')}

## By Category

| Category | Count |
|----------|-------|
"""
    for cat in sorted(stats['by_category'].keys()):
        count = stats['by_category'][cat]
        report += f"| {cat} | {count} |\n"

    report += "\n## Recent Imports\n\n"
    recent = sorted(stats['conversations'], key=lambda x: x['created'], reverse=True)[:20]
    for conv in recent:
        report += f"- **{conv['category']}**: {conv['name']} â†’ `{conv['file']}`\n"

    return report


def main():
    parser = argparse.ArgumentParser(description='Import Claude export data to Obsidian vaults')
    parser.add_argument('export_dir', help='Path to Claude export directory')
    parser.add_argument('--dry-run', '-n', action='store_true',
                       help='Show what would be done without making changes')
    parser.add_argument('--base-dir', '-b', default=None,
                       help='Base directory for Obsidian vaults (default: parent of export_dir)')
    args = parser.parse_args()

    export_dir = Path(args.export_dir)
    if args.base_dir:
        base_dir = Path(args.base_dir)
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: export_dir ã®è¦ªã®è¦ªï¼ˆ@index/claude/xxx â†’ Obsidian/ï¼‰
        base_dir = export_dir.parent.parent.parent

    if not export_dir.exists():
        print(f"Error: Directory not found: {export_dir}")
        return 1

    print(f"{'[DRY RUN] ' if args.dry_run else ''}Importing Claude export from: {export_dir}")
    print(f"Base directory: {base_dir}")

    # conversations.json
    conv_file = export_dir / 'conversations.json'
    if not conv_file.exists():
        print(f"Error: conversations.json not found in {export_dir}")
        return 1

    print("Loading conversations...")
    with open(conv_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    print(f"Processing {len(data)} conversations...")
    stats = process_conversations(data, base_dir, dry_run=args.dry_run)

    # ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
    print("\n=== Import Summary ===")
    for cat in sorted(stats['by_category'].keys()):
        count = stats['by_category'][cat]
        print(f"  {cat}: {count}")

    # ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    report = generate_import_report(stats, base_dir, dry_run=args.dry_run)
    report_file = base_dir / '@index' / 'claude' / 'import_report.md'
    if not args.dry_run:
        report_file.parent.mkdir(parents=True, exist_ok=True)
        report_file.write_text(report, encoding='utf-8')
        print(f"\nReport saved to: {report_file}")

    print(f"\n{'[DRY RUN] ' if args.dry_run else ''}Done!")
    return 0


if __name__ == '__main__':
    exit(main())
