#!/usr/bin/env python3
"""
Tag Extractor - æ—¢å­˜Vaultã‹ã‚‰ã‚¿ã‚°ã‚’FR-004ã«åŸºã¥ã„ã¦æŠ½å‡ºãƒ»ã‚«ãƒ†ã‚´ãƒªåˆ†é¡

Usage:
    python3 tag_extractor.py [options]

Options:
    --output, -o    å‡ºåŠ›å…ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: .claude/scripts/data/tag_dictionary.jsonï¼‰
    --limit         å„ã‚«ãƒ†ã‚´ãƒªã®æœ€å¤§ã‚¿ã‚°æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 30ï¼‰
    --vaults        å¯¾è±¡Vaultï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: å…¨Vaultï¼‰
    --json          JSONå½¢å¼ã§å‡ºåŠ›
"""

from __future__ import annotations

import argparse
import json
import re
from collections import Counter
from datetime import datetime
from pathlib import Path
from typing import TypedDict


# =============================================================================
# Configuration
# =============================================================================

OBSIDIAN_ROOT = Path(__file__).parent.parent
DEFAULT_OUTPUT = OBSIDIAN_ROOT / "scripts/data/tag_dictionary.json"
DEFAULT_LIMIT = 30

# Vault directories to scan
VAULT_DIRS = ["ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢", "ãƒ“ã‚¸ãƒã‚¹", "çµŒæ¸ˆ", "æ—¥å¸¸", "ãã®ä»–"]

# Tag categorization keywords
CATEGORY_KEYWORDS = {
    "languages": [
        "python", "ruby", "rails", "golang", "go", "javascript", "typescript",
        "java", "kotlin", "swift", "rust", "c", "cpp", "csharp", "php",
        "scala", "elixir", "haskell", "clojure", "lua", "perl", "r",
        "nodejs", "react", "vue", "angular", "django", "flask", "spring",
        "nextjs", "nuxt", "svelte", "express", "fastapi", "gin"
    ],
    "infrastructure": [
        "aws", "gcp", "azure", "docker", "kubernetes", "k8s", "linux",
        "nginx", "apache", "terraform", "ansible", "jenkins", "circleci",
        "github-actions", "gitlab-ci", "prometheus", "grafana", "elasticsearch",
        "redis", "postgresql", "mysql", "mongodb", "dynamodb", "s3",
        "ec2", "lambda", "ecs", "eks", "cloudflare", "vercel", "netlify"
    ],
    "tools": [
        "git", "bash", "vim", "neovim", "emacs", "vscode", "ssh", "tmux",
        "make", "cmake", "webpack", "vite", "npm", "yarn", "pnpm",
        "pip", "poetry", "cargo", "homebrew", "apt", "yum"
    ],
    "concepts": [
        "api", "rest", "graphql", "grpc", "security", "authentication",
        "authorization", "oauth", "jwt", "ssl", "tls", "encryption",
        "testing", "tdd", "ci", "cd", "devops", "agile", "scrum",
        "microservices", "monolith", "architecture", "design-pattern",
        "performance", "optimization", "caching", "logging", "monitoring"
    ],
    "lifestyle": [
        "æ—…è¡Œ", "æ–™ç†", "å¥åº·", "é‹å‹•", "è¶£å‘³", "èª­æ›¸", "æ˜ ç”»", "éŸ³æ¥½",
        "ã‚²ãƒ¼ãƒ ", "å†™çœŸ", "ã‚¢ãƒ¼ãƒˆ", "ãƒ‡ã‚¶ã‚¤ãƒ³", "ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³", "ç¾å®¹",
        "è©±ã—æ–¹", "ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³", "ç¿’æ…£", "ç”Ÿæ´»", "å®¶äº‹"
    ]
}

# Tags to exclude (auto-generated or not useful)
EXCLUDE_TAGS = {
    "conversation", "claude-export", "draft", "wip", "todo",
    "æœªæ•´ç†", "è¦ç¢ºèª", "imported"
}


# =============================================================================
# Type Definitions
# =============================================================================

class TagDictionary(TypedDict):
    languages: list[str]
    infrastructure: list[str]
    tools: list[str]
    concepts: list[str]
    lifestyle: list[str]
    total_count: int
    extracted_at: str
    source_vaults: list[str]


# =============================================================================
# Core Functions
# =============================================================================

def normalize_tag(tag: str) -> str:
    """ã‚¿ã‚°ã‚’æ­£è¦åŒ–ï¼ˆå°æ–‡å­—åŒ–ã€ãƒã‚¤ãƒ•ãƒ³çµ±ä¸€ï¼‰"""
    # å°æ–‡å­—åŒ–
    tag = tag.lower().strip()
    # ã‚¹ãƒšãƒ¼ã‚¹ã‚’ãƒã‚¤ãƒ•ãƒ³ã«
    tag = tag.replace(" ", "-")
    # é€£ç¶šãƒã‚¤ãƒ•ãƒ³ã‚’å˜ä¸€ã«
    tag = re.sub(r"-+", "-", tag)
    # å…ˆé ­æœ«å°¾ã®ãƒã‚¤ãƒ•ãƒ³é™¤å»
    tag = tag.strip("-")
    return tag


def extract_frontmatter_tags(content: str) -> list[str]:
    """Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰frontmatterã®tagsã‚’æŠ½å‡º"""
    # YAML frontmatteræŠ½å‡º
    match = re.match(r"^---\s*\n(.*?)\n---", content, re.DOTALL)
    if not match:
        return []

    frontmatter = match.group(1)
    tags = []

    # tags: ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æ¤œç´¢
    # ãƒ‘ã‚¿ãƒ¼ãƒ³1: tags: [tag1, tag2, tag3]
    inline_match = re.search(r"tags:\s*\[(.*?)\]", frontmatter)
    if inline_match:
        tag_str = inline_match.group(1)
        tags = [t.strip().strip("'\"") for t in tag_str.split(",")]
    else:
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: tags:\n  - tag1\n  - tag2
        in_tags_block = False
        for line in frontmatter.split("\n"):
            if line.startswith("tags:"):
                in_tags_block = True
                # åŒä¸€è¡Œã«ã‚¿ã‚°ãŒã‚ã‚‹å ´åˆ
                remaining = line[5:].strip()
                if remaining and not remaining.startswith("-"):
                    tags.append(remaining.strip("'\""))
            elif in_tags_block:
                if line.strip().startswith("-"):
                    tag = line.strip().lstrip("-").strip().strip("'\"")
                    if tag:
                        tags.append(tag)
                elif line.strip() and not line.startswith(" ") and not line.startswith("\t"):
                    # åˆ¥ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«åˆ°é”
                    break

    return [normalize_tag(t) for t in tags if t]


def categorize_tag(tag: str) -> str | None:
    """ã‚¿ã‚°ã‚’ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡"""
    tag_lower = tag.lower()

    for category, keywords in CATEGORY_KEYWORDS.items():
        if tag_lower in keywords:
            return category
        # éƒ¨åˆ†ä¸€è‡´ã‚‚è¨±å®¹ï¼ˆä¾‹: "rails" in "ruby-on-rails"ï¼‰
        for keyword in keywords:
            if keyword in tag_lower or tag_lower in keyword:
                return category

    # ã‚«ãƒ†ã‚´ãƒªä¸æ˜ã®å ´åˆ
    # æ—¥æœ¬èªã‚’å«ã‚€å ´åˆã¯lifestyleå€™è£œ
    if re.search(r"[\u3040-\u309f\u30a0-\u30ff\u4e00-\u9fff]", tag):
        return "lifestyle"

    # è‹±èªã®ã¿ãªã‚‰conceptså€™è£œ
    return "concepts"


def scan_vault(vault_path: Path) -> Counter:
    """Vaultãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã¦ã‚¿ã‚°ã‚’åé›†"""
    tag_counter: Counter = Counter()

    if not vault_path.exists():
        return tag_counter

    for md_file in vault_path.rglob("*.md"):
        try:
            content = md_file.read_text(encoding="utf-8")
            tags = extract_frontmatter_tags(content)
            for tag in tags:
                if tag and tag not in EXCLUDE_TAGS:
                    tag_counter[tag] += 1
        except (UnicodeDecodeError, OSError):
            continue

    return tag_counter


def extract_tag_dictionary(
    vaults: list[str] | None = None,
    limit: int = DEFAULT_LIMIT
) -> TagDictionary:
    """Vaultã‹ã‚‰ã‚¿ã‚°è¾æ›¸ã‚’æŠ½å‡º"""
    if vaults is None:
        vaults = VAULT_DIRS

    # å…¨ã‚¿ã‚°ã‚’åé›†
    all_tags: Counter = Counter()
    for vault_name in vaults:
        vault_path = OBSIDIAN_ROOT / vault_name
        vault_tags = scan_vault(vault_path)
        all_tags.update(vault_tags)

    # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«åˆ†é¡
    categorized: dict[str, Counter] = {
        "languages": Counter(),
        "infrastructure": Counter(),
        "tools": Counter(),
        "concepts": Counter(),
        "lifestyle": Counter()
    }

    for tag, count in all_tags.items():
        category = categorize_tag(tag)
        if category:
            categorized[category][tag] = count

    # å„ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰é »åº¦ä¸Šä½ã‚’æŠ½å‡º
    result: TagDictionary = {
        "languages": [t for t, _ in categorized["languages"].most_common(limit)],
        "infrastructure": [t for t, _ in categorized["infrastructure"].most_common(limit)],
        "tools": [t for t, _ in categorized["tools"].most_common(limit)],
        "concepts": [t for t, _ in categorized["concepts"].most_common(limit)],
        "lifestyle": [t for t, _ in categorized["lifestyle"].most_common(limit)],
        "total_count": sum(len(v) for v in categorized.values()),
        "extracted_at": datetime.now().isoformat(timespec="seconds"),
        "source_vaults": vaults
    }

    return result


# =============================================================================
# CLI
# =============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="æ—¢å­˜Vaultã‹ã‚‰ã‚¿ã‚°è¾æ›¸ã‚’æŠ½å‡º"
    )
    parser.add_argument(
        "-o", "--output",
        type=Path,
        default=DEFAULT_OUTPUT,
        help=f"å‡ºåŠ›å…ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {DEFAULT_OUTPUT}ï¼‰"
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=DEFAULT_LIMIT,
        help=f"å„ã‚«ãƒ†ã‚´ãƒªã®æœ€å¤§ã‚¿ã‚°æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {DEFAULT_LIMIT}ï¼‰"
    )
    parser.add_argument(
        "--vaults",
        type=str,
        default=None,
        help="å¯¾è±¡Vaultï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: å…¨Vaultï¼‰"
    )
    parser.add_argument(
        "--json",
        action="store_true",
        help="JSONå½¢å¼ã§æ¨™æº–å‡ºåŠ›"
    )

    args = parser.parse_args()

    # VaultæŒ‡å®šã®ãƒ‘ãƒ¼ã‚¹
    vaults = None
    if args.vaults:
        vaults = [v.strip() for v in args.vaults.split(",")]

    # æŠ½å‡ºå®Ÿè¡Œ
    result = extract_tag_dictionary(vaults=vaults, limit=args.limit)

    # å‡ºåŠ›
    if args.json:
        print(json.dumps(result, ensure_ascii=False, indent=2))
    else:
        # äººé–“å¯èª­å½¢å¼
        print("ğŸ“Š ã‚¿ã‚°è¾æ›¸æŠ½å‡ºå®Œäº†")
        print(f"  è¨€èª: {len(result['languages'])} ã‚¿ã‚° ({', '.join(result['languages'][:5])}...)")
        print(f"  ã‚¤ãƒ³ãƒ•ãƒ©: {len(result['infrastructure'])} ã‚¿ã‚° ({', '.join(result['infrastructure'][:5])}...)")
        print(f"  ãƒ„ãƒ¼ãƒ«: {len(result['tools'])} ã‚¿ã‚° ({', '.join(result['tools'][:5])}...)")
        print(f"  æ¦‚å¿µ: {len(result['concepts'])} ã‚¿ã‚° ({', '.join(result['concepts'][:5])}...)")
        print(f"  æ—¥å¸¸: {len(result['lifestyle'])} ã‚¿ã‚° ({', '.join(result['lifestyle'][:5])}...)")
        print("  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        print(f"  åˆè¨ˆ: {result['total_count']} ã‚¿ã‚°")

        # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
        args.output.parent.mkdir(parents=True, exist_ok=True)
        with open(args.output, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"  å‡ºåŠ›: {args.output}")


if __name__ == "__main__":
    main()
