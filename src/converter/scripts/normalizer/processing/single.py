"""
Single - å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†

ãƒ•ã‚¡ã‚¤ãƒ«åæ­£è¦åŒ–ã€æ­£è¦åŒ–å®Ÿè¡Œã€å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚’è¡Œã†ã€‚
"""
from __future__ import annotations

import hashlib
import re
from pathlib import Path
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    pass

from normalizer.models import (
    NormalizationResult,
    ProcessingResult,
)
from normalizer.config import (
    BASE_DIR,
)
from normalizer.validators.title import validate_title, log_title_quality
from normalizer.validators.format import validate_markdown_format, log_format_quality
from normalizer.validators.tags import normalize_tags, calculate_tag_consistency, log_tag_quality
from normalizer.detection.english import log_english_detection
from normalizer.io.files import get_destination_path, read_file_content, write_file_content
from normalizer.io.session import timestamp, log_message
from normalizer.pipeline.runner import run_pipeline_v2


# =============================================================================
# File ID Generation
# =============================================================================


def generate_file_id(content: str, filepath: Path) -> str:
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¨åˆå›ãƒ‘ã‚¹ã‹ã‚‰ãƒãƒƒã‚·ãƒ¥IDã‚’ç”Ÿæˆ

    Args:
        content: ãƒ•ã‚¡ã‚¤ãƒ«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
        filepath: ãƒ•ã‚¡ã‚¤ãƒ«ã®ç›¸å¯¾ãƒ‘ã‚¹ï¼ˆåˆå›å‡¦ç†æ™‚ã®ãƒ‘ã‚¹ï¼‰

    Returns:
        12æ–‡å­—ã®16é€²æ•°ãƒãƒƒã‚·ãƒ¥IDï¼ˆSHA-256ã®å…ˆé ­48ãƒ“ãƒƒãƒˆï¼‰
    """
    # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ + ç›¸å¯¾ãƒ‘ã‚¹ã‚’çµåˆã—ã¦ãƒãƒƒã‚·ãƒ¥åŒ–
    # ãƒ‘ã‚¹ã¯ POSIX å½¢å¼ã«æ­£è¦åŒ–ï¼ˆã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å¯¾å¿œï¼‰
    path_str = filepath.as_posix()
    combined = f"{content}\n---\n{path_str}"
    hash_digest = hashlib.sha256(combined.encode("utf-8")).hexdigest()
    return hash_digest[:12]


def extract_file_id_from_frontmatter(content: str) -> str | None:
    """Markdown ãƒ•ã‚¡ã‚¤ãƒ«ã® frontmatter ã‹ã‚‰ file_id ã‚’æŠ½å‡ºã™ã‚‹ (T028)

    Args:
        content: Markdown ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹

    Returns:
        file_id (12æ–‡å­—ã®16é€²æ•°) ã¾ãŸã¯ None

    Example:
        >>> content = "---\\ntitle: Test\\nfile_id: a1b2c3d4e5f6\\n---\\n# Content"
        >>> extract_file_id_from_frontmatter(content)
        'a1b2c3d4e5f6'
    """
    # frontmatter ã‚’æŠ½å‡º (--- ã§å›²ã¾ã‚ŒãŸéƒ¨åˆ†)
    frontmatter_match = re.match(r"^---\n(.*?)\n---", content, re.DOTALL)
    if not frontmatter_match:
        return None

    frontmatter = frontmatter_match.group(1)

    # file_id ã‚’æŠ½å‡º (12æ–‡å­—ã®16é€²æ•°å°æ–‡å­—ã®ã¿)
    file_id_match = re.search(r"^file_id:\s*([a-f0-9]{12})\s*$", frontmatter, re.MULTILINE)
    if file_id_match:
        return file_id_match.group(1)

    return None


def get_or_generate_file_id(content: str, filepath: Path) -> str:
    """æ—¢å­˜ã® file_id ã‚’ç¶­æŒã€ãªã‘ã‚Œã°æ–°è¦ç”Ÿæˆ (T029)

    ã€Œfile_id ãŒãªã‘ã‚Œã°ç”Ÿæˆã€ã‚ã‚Œã°ç¶­æŒã€ã®åŸå‰‡ã«å¾“ã†ã€‚

    Args:
        content: ãƒ•ã‚¡ã‚¤ãƒ«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
        filepath: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹

    Returns:
        file_id (12æ–‡å­—ã®16é€²æ•°)
    """
    # æ—¢å­˜ã® file_id ã‚’æŠ½å‡º
    existing_file_id = extract_file_id_from_frontmatter(content)
    if existing_file_id:
        return existing_file_id

    # æ–°è¦ç”Ÿæˆ
    return generate_file_id(content, filepath)


# =============================================================================
# Filename Utilities
# =============================================================================


def clean_filename(filename: str) -> str:
    """ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ—¥ä»˜ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’é™¤å»

    Args:
        filename: å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆæ‹¡å¼µå­ãªã—ï¼‰

    Returns:
        ã‚¯ãƒªãƒ¼ãƒ³ãªãƒ•ã‚¡ã‚¤ãƒ«å
    """
    # Jekyllå½¢å¼ã®æ—¥ä»˜ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’é™¤å»: 2022-10-17-Title â†’ Title
    cleaned = re.sub(r'^\d{4}[-_]\d{2}[-_]\d{2}[-_]', '', filename)
    return cleaned


def normalize_filename(title: str) -> str:
    """ã‚¿ã‚¤ãƒˆãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ«åã¨ã—ã¦ä½¿ç”¨å¯èƒ½ãªå½¢å¼ã«æ­£è¦åŒ–

    Args:
        title: å…ƒã®ã‚¿ã‚¤ãƒˆãƒ«

    Returns:
        æ­£è¦åŒ–ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆæ‹¡å¼µå­ãªã—ï¼‰
    """
    if not title:
        return ""

    # ç¦æ­¢æ–‡å­—ã‚’é™¤å»/ç½®æ›
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã§ç¦æ­¢ã•ã‚Œã‚‹æ–‡å­—: / \\ : * ? " < > |
    normalized = re.sub(r'[/\\:*?"<>|]', '', title)

    # è¤‡æ•°ã®ç©ºç™½ã‚’1ã¤ã«
    normalized = re.sub(r'\s+', ' ', normalized)

    # å‰å¾Œã®ç©ºç™½ã‚’é™¤å»
    normalized = normalized.strip()

    return normalized


# =============================================================================
# File Building
# =============================================================================


def extract_frontmatter(content: str) -> tuple[dict | None, str]:
    """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰frontmatterã‚’æŠ½å‡º

    Args:
        content: Markdownã‚³ãƒ³ãƒ†ãƒ³ãƒ„

    Returns:
        tuple: (frontmatter_dict, body_content)
    """
    if not content.startswith('---'):
        return None, content

    # frontmatterã®çµ‚ç«¯ã‚’æ¢ã™
    end_match = re.search(r'\n---\s*\n', content[3:])
    if not end_match:
        return None, content

    fm_end = end_match.end() + 3
    fm_content = content[4:fm_end - 4]
    body = content[fm_end:]

    # ç°¡æ˜“çš„ãªYAMLãƒ‘ãƒ¼ã‚¹
    fm = {}
    for line in fm_content.split('\n'):
        if ':' in line:
            key, _, value = line.partition(':')
            fm[key.strip()] = value.strip()

    return fm, body


def build_normalized_file(
    result: NormalizationResult,
    file_id: str | None = None
) -> str:
    """æ­£è¦åŒ–çµæœã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’æ§‹ç¯‰

    Args:
        result: æ­£è¦åŒ–çµæœ
        file_id: ãƒ•ã‚¡ã‚¤ãƒ«è¿½è·¡ç”¨ãƒãƒƒã‚·ãƒ¥ID

    Returns:
        Markdownãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹
    """
    fm = result["frontmatter"]

    # frontmatteræ§‹ç¯‰
    lines = ["---"]
    lines.append(f'title: "{fm["title"]}"')

    if fm["tags"]:
        lines.append("tags:")
        for tag in fm["tags"]:
            lines.append(f"  - {tag}")

    if fm["created"]:
        lines.append(f"created: {fm['created']}")

    if fm.get("summary"):
        lines.append(f'summary: "{fm["summary"]}"')

    if fm.get("related"):
        lines.append("related:")
        for rel in fm["related"]:
            lines.append(f'  - "{rel}"')

    # ãƒ•ã‚¡ã‚¤ãƒ«è¿½è·¡IDè¿½åŠ 
    if file_id is not None:
        lines.append(f"file_id: {file_id}")

    lines.append("---")
    lines.append("")

    # æœ¬æ–‡è¿½åŠ ï¼ˆæ—¢å­˜frontmatterã‚’é™¤å»ã—ã¦ä¿è­·ï¼‰
    body = result["normalized_content"]
    existing_fm, body_only = extract_frontmatter(body)
    if existing_fm:
        body = body_only.strip()

    lines.append(body)

    return "\n".join(lines)


# =============================================================================
# Markdown Normalization
# =============================================================================


def normalize_markdown(content: str) -> str:
    """Markdownã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ­£è¦åŒ–

    Args:
        content: å…ƒã®Markdownã‚³ãƒ³ãƒ†ãƒ³ãƒ„

    Returns:
        æ­£è¦åŒ–ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„
    """
    # è¤‡æ•°ã®ç©ºè¡Œã‚’1ã¤ã«
    normalized = re.sub(r'\n{3,}', '\n\n', content)

    # æœ«å°¾ã®ç©ºç™½ã‚’é™¤å»
    lines = [line.rstrip() for line in normalized.split('\n')]
    normalized = '\n'.join(lines)

    # æœ«å°¾ã«æ”¹è¡Œã‚’è¿½åŠ 
    if not normalized.endswith('\n'):
        normalized += '\n'

    return normalized


# =============================================================================
# File Normalization
# =============================================================================


def normalize_file(filepath: Path) -> tuple[NormalizationResult | None, str | None]:
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’Ollamaã§åˆ†é¡ãƒ»æ­£è¦åŒ–

    Args:
        filepath: å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹

    Returns:
        tuple: (NormalizationResult, error_message)
    """
    # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    content, err = read_file_content(filepath)
    if err:
        return None, err

    # Multi-stage pipeline (Aâ†’Bâ†’C) ã‚’å®Ÿè¡Œ
    result = run_pipeline_v2(filepath, content)

    # è‹±èªæ–‡æ›¸åˆ¤å®šãƒ­ã‚°è¨˜éŒ²
    log_english_detection(
        filepath.name,
        result["is_complete_english_doc"],
        0.0,  # ã‚¹ã‚³ã‚¢ã¯pipelineå†…ã§è¨ˆç®—æ¸ˆã¿
        {}
    )

    # ã‚¿ã‚¤ãƒˆãƒ«å“è³ªæ¤œè¨¼ãƒ»ä¿®æ­£
    title = result["frontmatter"]["title"]
    is_valid, issues = validate_title(title)

    if not is_valid:
        # ã‚¿ã‚¤ãƒˆãƒ«ãŒç„¡åŠ¹ãªå ´åˆã€æ­£è¦åŒ–ã‚’è©¦ã¿ã‚‹
        cleaned_title = normalize_filename(title)
        if cleaned_title:
            result["frontmatter"]["title"] = cleaned_title
        else:
            # æ­£è¦åŒ–ã§ã‚‚ç©ºã«ãªã‚‹å ´åˆã¯ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            result["frontmatter"]["title"] = clean_filename(filepath.stem)

    # ã‚¿ã‚¤ãƒˆãƒ«å“è³ªãƒ­ã‚°è¨˜éŒ²
    log_title_quality(filepath.name, result["frontmatter"]["title"])

    # ã‚¿ã‚°å“è³ªæ¤œè¨¼ãƒ»ä¿®æ­£
    tags = result["frontmatter"]["tags"]
    tags = normalize_tags(tags)
    result["frontmatter"]["tags"] = tags

    # ã‚¿ã‚°ä¸€è²«æ€§è¨ˆç®—
    consistency_rate, matched, unmatched = calculate_tag_consistency(tags)

    # ã‚¿ã‚°å“è³ªãƒ­ã‚°è¨˜éŒ²
    log_tag_quality(filepath.name, tags, consistency_rate, matched, unmatched)

    # Markdownæ­£è¦åŒ–ã‚’post-processingã¨ã—ã¦é©ç”¨
    normalized_content = result["normalized_content"]
    if normalized_content:
        result["normalized_content"] = normalize_markdown(normalized_content)

    return result, None


# =============================================================================
# Single File Processing
# =============================================================================


def process_single_file(
    filepath: Path,
    preview: bool = False,
    quiet: bool = False,
    output_json: bool = False
) -> ProcessingResult:
    """å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†

    Args:
        filepath: å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«
        preview: ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ¢ãƒ¼ãƒ‰ï¼ˆç§»å‹•ã—ãªã„ï¼‰
        quiet: é€²æ—è¡¨ç¤ºæŠ‘åˆ¶
        output_json: JSONå‡ºåŠ›

    Returns:
        ProcessingResult
    """
    import json

    # fixtures ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯è‡ªå‹•çš„ã«previewãƒ¢ãƒ¼ãƒ‰ã‚’å¼·åˆ¶
    if "tests/fixtures/" in str(filepath):
        preview = True
        if not quiet:
            print("âš ï¸ fixturesãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãŸã‚è‡ªå‹•çš„ã«previewãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ")

    result: ProcessingResult = {
        "success": False,
        "file": str(filepath),
        "genre": None,
        "confidence": "low",
        "destination": None,
        "error": None,
        "timestamp": timestamp(),
        "original_chars": None,
        "normalized_chars": None,
        "char_diff": None,
        "improvements_made": None,
        "is_complete_english_doc": None,
        "file_id": None
    }

    # å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã®æ–‡å­—æ•°ã‚’å–å¾— & file_id å–å¾—/ç”Ÿæˆ (T030)
    try:
        original_content = filepath.read_text(encoding="utf-8")
        result["original_chars"] = len(original_content)
        # ãƒ•ã‚¡ã‚¤ãƒ«è¿½è·¡ç”¨ãƒãƒƒã‚·ãƒ¥ID: æ—¢å­˜ã‚’ç¶­æŒã€ãªã‘ã‚Œã°ç”Ÿæˆ
        result["file_id"] = get_or_generate_file_id(original_content, filepath)
    except Exception:
        pass

    # æ­£è¦åŒ–å®Ÿè¡Œ
    norm_result, err = normalize_file(filepath)
    if err:
        result["error"] = err
        if output_json:
            print(json.dumps(result, ensure_ascii=False, indent=2))
        else:
            log_message(f"âŒ ã‚¨ãƒ©ãƒ¼\n  ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«: {result['file']}\n  ğŸ’¥ ã‚¨ãƒ©ãƒ¼: {result['error']}")
        return result

    result["genre"] = norm_result["genre"]
    confidence = norm_result["confidence"]
    result["confidence"] = confidence
    result["improvements_made"] = norm_result.get("improvements_made", [])
    result["is_complete_english_doc"] = norm_result.get("is_complete_english_doc", False)

    # ç§»å‹•å…ˆæ±ºå®š
    title = norm_result["frontmatter"]["title"]
    normalized_title = normalize_filename(title)

    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚¿ã‚¤ãƒˆãƒ«ãŒç©ºã®å ´åˆã¯å…ƒãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ç”Ÿæˆ
    if not normalized_title:
        fallback_name = clean_filename(filepath.stem)
        normalized_title = fallback_name.replace('-', ' ')

    new_filename = normalized_title + ".md"
    dest_path = get_destination_path(norm_result["genre"], new_filename, norm_result.get("subfolder", ""))

    result["destination"] = str(dest_path.relative_to(BASE_DIR))

    if preview:
        # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ¢ãƒ¼ãƒ‰ - æ–‡å­—æ•°ã‚‚è¨ˆç®—
        preview_content = build_normalized_file(
            norm_result,
            file_id=result["file_id"]
        )
        result["normalized_chars"] = len(preview_content)
        if result["original_chars"] is not None:
            result["char_diff"] = result["normalized_chars"] - result["original_chars"]

        result["success"] = True
        if output_json:
            print(json.dumps(result, ensure_ascii=False, indent=2))
        elif not quiet:
            _log_processing_result(result, norm_result)
            log_message("  ğŸ‘ï¸ ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ¢ãƒ¼ãƒ‰ï¼ˆç§»å‹•ãªã—ï¼‰")
        return result

    # ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿
    normalized_content = build_normalized_file(
        norm_result,
        file_id=result["file_id"]
    )
    write_err = write_file_content(dest_path, normalized_content)
    if write_err:
        result["error"] = write_err
        if output_json:
            print(json.dumps(result, ensure_ascii=False, indent=2))
        else:
            log_message(f"âŒ ã‚¨ãƒ©ãƒ¼\n  ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«: {result['file']}\n  ğŸ’¥ ã‚¨ãƒ©ãƒ¼: {result['error']}")
        return result

    # æ–‡å­—æ•°çµ±è¨ˆã‚’è¨˜éŒ²
    result["normalized_chars"] = len(normalized_content)
    if result["original_chars"] is not None:
        result["char_diff"] = result["normalized_chars"] - result["original_chars"]

    # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¤œè¨¼
    format_valid, format_issues = validate_markdown_format(normalized_content)
    log_format_quality(new_filename, normalized_content, format_valid, format_issues)

    # å…ƒãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
    try:
        filepath.unlink()
    except Exception as e:
        result["error"] = f"å…ƒãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}"
        if output_json:
            print(json.dumps(result, ensure_ascii=False, indent=2))
        else:
            log_message(f"âŒ ã‚¨ãƒ©ãƒ¼\n  ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«: {result['file']}\n  ğŸ’¥ ã‚¨ãƒ©ãƒ¼: {result['error']}")
        return result

    result["success"] = True

    # çµæœå‡ºåŠ›
    if output_json:
        print(json.dumps(result, ensure_ascii=False, indent=2))
    elif not quiet:
        _log_processing_result(result, norm_result)

    return result


def _log_processing_result(
    result: ProcessingResult,
    norm_result: NormalizationResult,
) -> None:
    """å‡¦ç†çµæœã‚’ãƒ­ã‚°ã«å‡ºåŠ›ï¼ˆå†…éƒ¨ãƒ˜ãƒ«ãƒ‘ãƒ¼ï¼‰"""
    if norm_result["genre"] == "dust":
        log_message(f"""ğŸ—‘ï¸ Duståˆ¤å®š
  ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«: {result['file']}
  ğŸ“‚ ç§»å‹•å…ˆ: {result['destination']}
  ğŸ“ ç†ç”±: {norm_result['reason'] or 'ä¾¡å€¤ãªã—ã¨åˆ¤å®š'}""")
    else:
        lines = [
            "âœ… ãƒ•ã‚¡ã‚¤ãƒ«æ•´ç†å®Œäº†",
            f"  ğŸ“„ å…ƒãƒ•ã‚¡ã‚¤ãƒ«: {result['file']}",
            f"  ğŸ“‚ ç§»å‹•å…ˆ: {result['destination']}",
            f"  ğŸ·ï¸ ã‚¸ãƒ£ãƒ³ãƒ«: {result['genre']} (confidence: {result['confidence']:.2f})"
        ]
        if result.get("is_complete_english_doc"):
            lines.append("  ğŸŒ å®Œå…¨ãªè‹±èªæ–‡æ›¸ï¼ˆç¿»è¨³ãªã—ï¼‰")
        improvements = result.get("improvements_made", [])
        if improvements:
            lines.append(f"  âœ¨ æ”¹å–„ç‚¹ ({len(improvements)}ä»¶):")
            for imp in improvements[:3]:
                lines.append(f"    - {imp}")
            if len(improvements) > 3:
                lines.append(f"    ... ä»– {len(improvements) - 3} ä»¶")
        log_message("\n".join(lines))
